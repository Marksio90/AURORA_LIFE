# Apache Airflow Configuration
# This file is mounted into Airflow containers

[core]
# DAGs folder path
dags_folder = /opt/airflow/dags
# Plugins folder path
plugins_folder = /opt/airflow/plugins
# Executor - CeleryExecutor for distributed execution
executor = CeleryExecutor
# Parallelism - max tasks running across all workers
parallelism = 32
# DAG concurrency - max tasks per DAG
dag_concurrency = 16
# Max active DAG runs per DAG
max_active_runs_per_dag = 1
# Load examples (set to False in production)
load_examples = False
# Default timezone
default_timezone = UTC
# Enable colored logs
colored_console_log = True
# Enable log rotation
enable_xcom_pickling = True

[database]
# Database connection (set via environment variable)
# sql_alchemy_conn = postgresql+psycopg2://aurora:password@postgres:5432/aurora_life

[logging]
# Base log folder
base_log_folder = /opt/airflow/logs
# Remote logging (S3, GCS, etc.)
remote_logging = False
# Log level
logging_level = INFO
# FAB log level
fab_logging_level = WARNING
# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[celery]
# Celery broker URL (Redis)
# broker_url = redis://:password@redis:6379/1
# Celery result backend
# result_backend = redis://:password@redis:6379/1
# Worker concurrency
worker_concurrency = 8
# Task time limit (seconds)
task_time_limit = 7200
# Task soft time limit
task_soft_time_limit = 6600
# Worker log server port
worker_log_server_port = 8793

[scheduler]
# Scheduler interval (seconds)
scheduler_heartbeat_sec = 5
# Number of parsing processes
parsing_processes = 2
# DAG directory list interval (seconds)
dag_dir_list_interval = 300
# Min file process interval (seconds)
min_file_process_interval = 30
# Orphaned tasks check interval
orphaned_tasks_check_interval = 300
# Enable health check
enable_health_check = True

[webserver]
# Web server port
web_server_port = 8080
# Worker timeout (seconds)
web_server_worker_timeout = 120
# Workers
workers = 4
# Worker class
worker_class = sync
# Enable authentication
authenticate = True
# Auth backend
auth_backend = airflow.api.auth.backend.basic_auth
# Secret key (set via environment variable)
# secret_key = your-secret-key-here
# Expose config in UI
expose_config = False
# Base URL
base_url = http://localhost:8081

[api]
# Enable experimental API
enable_experimental_api = False
# Auth backend
auth_backend = airflow.api.auth.backend.basic_auth

[operators]
# Default owner for new DAGs
default_owner = aurora-ml
# Default queue
default_queue = default
# Default pool slots
default_pool_task_slot_count = 128

[email]
# Email backend
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# SMTP host
smtp_host = smtp.gmail.com
# SMTP starttls
smtp_starttls = True
# SMTP SSL
smtp_ssl = False
# SMTP port
smtp_port = 587
# SMTP user (set via environment variable)
# smtp_user = your-email@gmail.com
# SMTP password (set via environment variable)
# smtp_password = your-app-password
# SMTP mail from
smtp_mail_from = airflow@aurora-life.com

[metrics]
# Enable metrics
statsd_on = False
# StatsD host
statsd_host = localhost
# StatsD port
statsd_port = 8125
# StatsD prefix
statsd_prefix = airflow

[lineage]
# Backend for lineage
backend =

[atlas]
# Atlas host
sasl_enabled = False
host =
port = 21000
username =
password =

[kubernetes]
# Enable Kubernetes executor features
enable_tcp_keepalive = True
tcp_keepalive_intvl = 30
tcp_keepalive_probes = 6
tcp_keepalive_idle = 120
