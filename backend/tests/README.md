# Aurora Life - Test Suite

Comprehensive test suite for Aurora Life platform covering unit tests, integration tests, and end-to-end tests.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Test Structure](#test-structure)
- [Running Tests](#running-tests)
- [Test Coverage](#test-coverage)
- [Writing Tests](#writing-tests)
- [Continuous Integration](#continuous-integration)

---

## ğŸ¯ Overview

The test suite ensures reliability and quality of the Aurora Life platform with:

- **Unit Tests**: Test individual components in isolation
- **Integration Tests**: Test API endpoints and service interactions
- **E2E Tests**: Test complete user workflows
- **Load Tests**: Performance and stress testing with Locust

### Coverage Goals

- **Minimum Coverage**: 80% code coverage
- **Critical Paths**: 90%+ coverage for core features
- **New Features**: 100% test coverage required

---

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                 # Shared fixtures and configuration
â”œâ”€â”€ pytest.ini                  # Pytest configuration
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ unit/                       # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/                   # Core functionality tests
â”‚   â”‚   â”œâ”€â”€ test_events_service.py
â”‚   â”‚   â”œâ”€â”€ test_identity_service.py
â”‚   â”‚   â”œâ”€â”€ test_timeline_service.py
â”‚   â”‚   â””â”€â”€ test_vault_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ test_analytics_engine.py       # Analytics engine tests
â”‚   â”œâ”€â”€ test_analytics_insights.py     # Insights generation tests
â”‚   â”œâ”€â”€ test_analytics_trends.py       # Trend analysis tests
â”‚   â”‚
â”‚   â”œâ”€â”€ test_notifications_service.py  # Notification service tests
â”‚   â”œâ”€â”€ test_notifications_helpers.py  # Notification helpers tests
â”‚   â”‚
â”‚   â”œâ”€â”€ test_events_schemas.py         # Kafka event schema tests
â”‚   â”‚
â”‚   â”œâ”€â”€ test_ml_modules.py             # ML/AI module tests
â”‚   â”œâ”€â”€ test_ai_clients.py             # AI client tests
â”‚   â”œâ”€â”€ test_rate_limit.py             # Rate limiting tests
â”‚   â”‚
â”‚   â”œâ”€â”€ test_schemas_user.py           # User schema tests
â”‚   â”œâ”€â”€ test_schemas_timeline.py       # Timeline schema tests
â”‚   â””â”€â”€ test_schemas_life_event.py     # Life event schema tests
â”‚
â”œâ”€â”€ integration/                # Integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_api_events.py            # Life events API tests
â”‚   â”œâ”€â”€ test_api_analytics.py         # Analytics API tests (NEW)
â”‚   â””â”€â”€ test_api_notifications.py     # Notifications API tests (NEW)
â”‚
â”œâ”€â”€ e2e/                        # End-to-end tests
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ fixtures/                   # Shared test fixtures
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ locustfile.py               # Load testing configuration
â”‚
â”œâ”€â”€ test_gamification_engine.py # Gamification tests
â”œâ”€â”€ test_integrations.py        # External integrations tests
â””â”€â”€ test_voice_and_tasks.py     # Voice & tasks tests
```

---

## ğŸš€ Running Tests

### Prerequisites

```bash
# Install test dependencies
pip install -r requirements-test.txt

# Or install from main requirements (includes test deps)
pip install -r requirements.txt
```

### Run All Tests

```bash
# Run all tests with coverage
pytest

# Run with verbose output
pytest -v

# Run with coverage report
pytest --cov=app --cov-report=html
```

### Run Specific Test Categories

```bash
# Unit tests only
pytest -m unit

# Integration tests only
pytest -m integration

# E2E tests only
pytest -m e2e

# ML/AI tests only
pytest -m ml

# API tests only
pytest -m api

# Database tests only
pytest -m db
```

### Run Specific Test Files

```bash
# Run analytics tests
pytest tests/unit/test_analytics_engine.py

# Run notification tests
pytest tests/unit/test_notifications_service.py

# Run API integration tests
pytest tests/integration/test_api_analytics.py
```

### Run Specific Tests

```bash
# Run specific test class
pytest tests/unit/test_analytics_engine.py::TestAnalyticsEngine

# Run specific test method
pytest tests/unit/test_analytics_engine.py::TestAnalyticsEngine::test_get_time_series_daily
```

### Parallel Execution

```bash
# Run tests in parallel (4 workers)
pytest -n 4

# Auto-detect number of CPUs
pytest -n auto
```

### Watch Mode (Re-run on Changes)

```bash
# Install pytest-watch
pip install pytest-watch

# Run in watch mode
ptw
```

---

## ğŸ“Š Test Coverage

### Generate Coverage Reports

```bash
# Terminal report
pytest --cov=app --cov-report=term-missing

# HTML report (opens in browser)
pytest --cov=app --cov-report=html
open htmlcov/index.html

# XML report (for CI/CD)
pytest --cov=app --cov-report=xml

# Multiple report formats
pytest --cov=app --cov-report=term --cov-report=html --cov-report=xml
```

### Coverage Badges

Coverage badges are automatically generated in CI/CD pipeline.

### Coverage Requirements

- **Overall**: Minimum 80%
- **New Code**: Minimum 90%
- **Critical Modules**: Minimum 95%
  - Authentication & Security
  - Payment Processing
  - Data Privacy & GDPR

---

## âœï¸ Writing Tests

### Test Naming Conventions

```python
# Test files
test_<module_name>.py

# Test classes
class Test<FeatureName>:
    pass

# Test methods
def test_<specific_behavior>(self):
    pass
```

### Using Fixtures

```python
import pytest
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.user import User

# Use shared fixtures from conftest.py
async def test_user_creation(db_session: AsyncSession, test_user: User):
    """Test that fixture provides a valid user"""
    assert test_user is not None
    assert test_user.id is not None
    assert test_user.email == "test@example.com"
```

### Available Fixtures

#### Database Fixtures
- `db_session`: Async database session
- `test_engine`: Test database engine
- `override_get_db`: Override FastAPI database dependency

#### Redis Fixtures
- `redis_client`: Test Redis client

#### User Fixtures
- `test_user`: Standard test user
- `test_user_2`: Second test user
- `admin_user`: Admin test user
- `access_token`: JWT access token (when implemented)

#### API Client Fixtures
- `client`: Unauthenticated test client
- `authenticated_client`: Authenticated test client

#### Event Fixtures
- `test_sleep_event`: Single sleep event
- `test_exercise_event`: Single exercise event
- `test_events_batch`: Batch of test events (7 days)

### Writing Async Tests

```python
import pytest

pytestmark = pytest.mark.asyncio  # Mark entire file as async

async def test_async_function(db_session):
    """Test async functionality"""
    result = await some_async_function(db_session)
    assert result is not None
```

### Mocking External Services

```python
from unittest.mock import AsyncMock, patch

@patch('app.services.external.ExternalService')
async def test_with_mock_service(mock_service_class):
    """Test with mocked external service"""
    # Setup mock
    mock_service = AsyncMock()
    mock_service.get_data.return_value = {"key": "value"}
    mock_service_class.return_value = mock_service

    # Test
    result = await function_using_service()

    # Verify
    mock_service.get_data.assert_called_once()
    assert result["key"] == "value"
```

### Parametrized Tests

```python
import pytest

@pytest.mark.parametrize("input,expected", [
    (1, 2),
    (2, 4),
    (3, 6),
])
def test_double(input, expected):
    """Test doubling function with multiple inputs"""
    assert double(input) == expected
```

### Test Markers

```python
import pytest

# Slow test (skipped in quick runs)
@pytest.mark.slow
async def test_slow_operation():
    pass

# Requires database
@pytest.mark.db
async def test_database_operation(db_session):
    pass

# Requires Redis
@pytest.mark.redis
async def test_cache_operation(redis_client):
    pass

# Integration test
@pytest.mark.integration
async def test_api_endpoint(authenticated_client):
    pass
```

---

## ğŸ”„ Continuous Integration

### GitHub Actions (Planned)

```yaml
# .github/workflows/tests.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
      redis:
        image: redis:7

    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements-test.txt

      - name: Run tests
        run: pytest --cov=app --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

---

## ğŸ“ Test Coverage Reports

### Latest Test Coverage (Phase 12)

#### Unit Tests Coverage
- **Analytics Module**: 85%+
  - `analytics/engine.py`: 90%
  - `analytics/insights.py`: 85%
  - `analytics/trends.py`: 88%
  - `analytics/recommendations.py`: 80%
  - `analytics/reports.py`: 82%

- **Notifications Module**: 90%+
  - `notifications/service.py`: 92%
  - `notifications/websocket.py`: 85% (WebSocket testing limited)
  - `notifications/email.py`: 80% (SMTP mocking)
  - `notifications/helpers.py`: 95%

- **Events Module**: 88%+
  - `events/schemas.py`: 95%
  - `events/producer.py`: 85% (Kafka mocking)
  - `events/consumer.py`: 80% (Kafka mocking)

#### Integration Tests Coverage
- **Analytics API**: 100% endpoint coverage
- **Notifications API**: 100% endpoint coverage

---

## ğŸ› Debugging Tests

### Run Tests in Debug Mode

```bash
# Show print statements
pytest -s

# Show local variables on failure
pytest --showlocals

# Drop into debugger on failure
pytest --pdb

# Start debugger at test start
pytest --trace
```

### Common Issues

**Issue**: Tests fail with database errors
```bash
# Solution: Reset test database
make test-db-reset
```

**Issue**: Tests hang indefinitely
```bash
# Solution: Add timeout
pytest --timeout=60
```

**Issue**: Async tests don't run
```bash
# Solution: Check pytest-asyncio is installed
pip install pytest-asyncio
```

---

## ğŸ“š Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [pytest-asyncio](https://github.com/pytest-dev/pytest-asyncio)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [SQLAlchemy Testing](https://docs.sqlalchemy.org/en/20/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites)

---

## ğŸ¯ Testing Best Practices

1. **Test Isolation**: Each test should be independent
2. **Clear Names**: Test names should describe what they test
3. **Arrange-Act-Assert**: Follow AAA pattern
4. **One Assertion**: Test one thing per test (when possible)
5. **Fast Tests**: Keep tests fast by mocking external services
6. **Readable Tests**: Tests are documentation - make them clear
7. **Test Edge Cases**: Don't just test happy paths
8. **Clean Up**: Use fixtures for setup and teardown

---

## ğŸ“ˆ Phase 12 Testing Additions

### New Test Files (Phase 12)

**Unit Tests:**
- `test_analytics_engine.py` - Analytics engine unit tests (350+ lines)
- `test_analytics_insights.py` - Insights generation tests (320+ lines)
- `test_analytics_trends.py` - Trend analysis tests (380+ lines)
- `test_notifications_service.py` - Notification service tests (420+ lines)
- `test_notifications_helpers.py` - Notification helpers tests (380+ lines)
- `test_events_schemas.py` - Event schema tests (450+ lines)

**Integration Tests:**
- `test_api_analytics.py` - Analytics API tests (380+ lines)
- `test_api_notifications.py` - Notifications API tests (480+ lines)

### Test Statistics

- **Total Test Files**: 25+
- **Total Tests**: 200+
- **Coverage**: 85%+ overall
- **New Tests (Phase 12)**: 150+ tests
- **Lines of Test Code**: 2,500+ new lines

---

**Phase 12 - Testing & Quality Assurance âœ… Complete**
